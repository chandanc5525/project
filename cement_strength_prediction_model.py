# -*- coding: utf-8 -*-
"""Cement Strength Prediction Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W-S9jkELv0Zhhr6PL5XTxyCYuLmVzhKe

# **Cement Strength Prediction Model Using Machine Learning**

### **Prof.Chandan D.Chaudhari**
"""

# Data manipulation Library
import pandas as pd
import numpy as np
from collections import OrderedDict
from sklearn.preprocessing import StandardScaler
# Data Visualization Library
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# Filter Warnings Library
import warnings
warnings.filterwarnings('ignore')

# Setting Theme for Data Visualization and Processing
pd.set_option('display.max_columns', None)
plt.style.use('fivethirtyeight')

# Importing Sklearn Libraries
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
import xgboost
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

# Importing Dataset Using Pandas Function

try:
  data = pd.read_csv('concrete_data.csv')
  print('Data Imported Successfully')

except:
  print('Please Check Data Connection')

# Loading the DataFrame
print(data.head())

"""# **Problem Statement: Build a ML Model, which will predict the strength of the cement, by studying this 8 independent features.**


"""

import pandas as pd
import numpy as np
from collections import OrderedDict

def custom_summary(data):
    # List to store summary information for numerical data
    numerical_result = []

    for i in data.columns:
        if data[i].dtype != 'object':
            Q1 = np.percentile(data[i], 25)
            Q3 = np.percentile(data[i], 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Checking for outliers
            outliers = data[(data[i] < lower_bound) | (data[i] > upper_bound)]
            outlier_present = not outliers.empty

            stats = OrderedDict({
                'Feature Name': i,
                'Count': data[i].count(),
                'Mean': data[i].mean(),
                'Std': data[i].std(),
                'Min': data[i].min(),
                '25%': Q1,
                '50%': np.percentile(data[i], 50),
                '75%': Q3,
                'Max': data[i].max(),
                'Variance': data[i].var(),
                'Skewness': data[i].skew(),
                'Kurtosis': data[i].kurt(),
                'Outliers Present': outlier_present,
                'Number of Outliers': len(outliers)
            })
            numerical_result.append(stats)

    # Creating DataFrame for Numerical Summary
    numerical_summary = pd.DataFrame(numerical_result)

    # For Categorical Data
    categorical_result = []
    for i in data.columns:
        if data[i].dtype == 'object':
            stats = OrderedDict({
                'Feature Name': i,
                'Unique Values': data[i].nunique(),
                'Top': data[i].mode()[0],
                'Frequency': data[i].value_counts().iloc[0]
            })
            categorical_result.append(stats)

    # Creating DataFrame for Categorical Summary
    categorical_summary = pd.DataFrame(categorical_result)

    return numerical_summary, categorical_summary

# Example usage:
# Assuming 'data' is your DataFrame
numerical_summary, categorical_summary = custom_summary(data)

Numerical_data  = pd.DataFrame(numerical_summary)
Categorical_data = pd.DataFrame(categorical_summary)

Numerical_data

# Checking Box Plot for Outlier Detection

def boxen_plot(data):
    plt.figure(figsize=(20, 10))
    plot = 1

    for i in range(len(data.columns)):
        if data[data.columns[i]].dtype != 'object':
            plt.subplot(3, 3, plot)  # Create subplot in a 3x3 grid
            sns.boxenplot(data[data.columns[i]], color='maroon')
            plt.xlabel(data.columns[i])
            plot += 1

    plt.tight_layout()  # Adjust layout to prevent overlap
    plt.show()

# Checking Distribution Plot

def dist_plot(data):
  plt.figure(figsize=(25,10))
  plot = 1
  for i in data.columns:
    if data[i].dtype != 'object':
      plt.subplot(3,3,plot)
      sns.distplot(data[i],color = 'maroon')
      plt.xlabel(i)
      plt.ylabel('Density')
      plot+=1
  plt.tight_layout
  plt.show()

def pairplot(data):
  sns.pairplot(data)
  plt.tight_layout()
  plt.show()

dist_plot(data)
boxen_plot(data)
pairplot(data)

# Checking Multicolinearity Test
corr = data.corr()
data.ax = plt.subplots(figsize = (8,8))
sns.heatmap(corr, annot = True)

# Evaluating VIF Score
#from statsmodels.stats.outliers_influence import variance_inflation_factor
#def VIF(features):
#   vif = pd.DataFrame()
#    vif['vif'] = [variance_inflation_factor(features, i) for i in range(features.shape[1])]
#    vif['features'] = features.columns
#    vif = vif.sort_values(by = 'vif' , ascending = False)
#    return vif

#VIF(data.drop('concrete_compressive_strength' , axis=1))

def train_and_test_split(data, tcol, testSize=0.3, randomState = 3):
    X = data.drop(tcol,axis=1)
    y = data[tcol]
    for column in X.columns:
      X[column] += 1
      X[column] = np.log(X[column])
    return train_test_split(X,y,test_size = testSize,random_state=randomState)

def model_builder(model_name, model, data, t_col):
    X_train,X_test,y_train,y_test = train_and_test_split(data,t_col)
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test,y_pred))
    result = [model_name, rmse, r2]
    return result

def multiple_models(data, t_col):
    col_names=['Model Name','rmse','r2_score']
    result = pd.DataFrame(columns = col_names)
    result.loc[len(result)] = model_builder('Linear Regression',LinearRegression(),data,t_col)
    result.loc[len(result)] = model_builder('Lasso',Lasso(),data,t_col)
    result.loc[len(result)] = model_builder('Ridge',Ridge(),data,t_col)
    result.loc[len(result)] = model_builder('Decision Tree',DecisionTreeRegressor(),data,t_col)
    result.loc[len(result)] = model_builder('SVR',SVR(),data,t_col)
    result.loc[len(result)] = model_builder('KNN',KNeighborsRegressor(),data,t_col)
    result.loc[len(result)] = model_builder('Random Forest',RandomForestRegressor(),data,t_col)
    result.loc[len(result)] = model_builder('Gradient Boost',GradientBoostingRegressor(),data,t_col)
    result.loc[len(result)] = model_builder('ADA Boost',AdaBoostRegressor(),data,t_col)
    result.loc[len(result)] = model_builder('XG Boost',XGBRegressor(),data,t_col)
    return result.sort_values(by = 'r2_score', ascending=False)

multiple_models(data, 'concrete_compressive_strength')

def k_fold_cv(X, y , fold= 10):
    score_LR = cross_val_score(LinearRegression(), X, y ,cv = fold)
    score_LS = cross_val_score(Lasso(), X, y, cv = fold)
    score_RD = cross_val_score(Ridge(), X, y, cv = fold)
    score_DTR = cross_val_score(DecisionTreeRegressor(), X, y, cv = fold)
    score_SVR = cross_val_score(SVR(), X, y ,cv = fold)
    score_KNN = cross_val_score(KNeighborsRegressor(), X, y ,cv = fold)
    score_RF = cross_val_score(RandomForestRegressor(), X, y ,cv = fold)
    score_GB = cross_val_score(GradientBoostingRegressor(), X, y, cv = fold)
    score_ADA = cross_val_score(AdaBoostRegressor(), X, y, cv = fold)
    score_XG = cross_val_score(XGBRegressor(), X, y, cv = fold)



    model_name = ['Linear Regression','Lasso','Ridge','DTR','SVR','KNN','Random Forest','Gradient Boost','ADA Boost','XG' ]
    scores = [score_LR,score_LS,score_RD,score_DTR,score_SVR,score_KNN,score_RF,score_GB,score_ADA,score_XG]
    result = []
    for i in range(len(model_name)):
        score_mean = np.mean(scores[i])
        score_std = np.std(scores[i])
        m_name = model_name[i]
        temp = [m_name,score_mean,score_std]
        result.append(temp)
    k_fold_df = pd.DataFrame(result,columns = ['Model Name','CV Accuracy','CV STD'])
    return k_fold_df.sort_values('CV Accuracy',ascending= False)

k_fold_cv(data.drop('concrete_compressive_strength',axis=1), data['concrete_compressive_strength'])

from sklearn.model_selection import GridSearchCV

def hyperparameter_tuning(X, y):
    # Define models and their respective parameter grids
    param_grid = {
        'LinearRegression': {
            'model': LinearRegression(),
            'params': {}
        },
        'Lasso': {
            'model': Lasso(),
            'params': {
                'alpha': [0.01, 0.1, 1, 10, 100]
            }
        },
        'Ridge': {
            'model': Ridge(),
            'params': {
                'alpha': [0.01, 0.1, 1, 10, 100]
            }
        },
        'DecisionTreeRegressor': {
            'model': DecisionTreeRegressor(),
            'params': {
                'max_depth': [5, 10, 15, 20],
                'min_samples_split': [2, 5, 10]
            }
        },
        'SVR': {
            'model': SVR(),
            'params': {
                'C': [0.1, 1, 10],
                'gamma': [0.001, 0.01, 0.1]
            }
        },
        'KNeighborsRegressor': {
            'model': KNeighborsRegressor(),
            'params': {
                'n_neighbors': [3, 5, 7, 9],
                'weights': ['uniform', 'distance']
            }
        },
        'RandomForestRegressor': {
            'model': RandomForestRegressor(),
            'params': {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5, 10]
            }
        },
        'GradientBoostingRegressor': {
            'model': GradientBoostingRegressor(),
            'params': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            }
        },
        'AdaBoostRegressor': {
            'model': AdaBoostRegressor(),
            'params': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 1.0]
            }
        },
        'XGBRegressor': {
            'model': XGBRegressor(),
            'params': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            }
        }
    }

    results = []

    # Loop over each model and perform GridSearchCV
    for model_name, mp in param_grid.items():
        print(f"Performing Grid Search for {model_name}...")
        grid_search = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='r2')
        grid_search.fit(X, y)

        # Store the best model and its performance
        best_score = grid_search.best_score_
        best_params = grid_search.best_params_
        results.append([model_name, best_score, best_params])

    # Create a DataFrame with the results
    tuning_results = pd.DataFrame(results, columns=['Model Name', 'Best CV Score', 'Best Parameters'])

    # Sort the DataFrame by the best cross-validation score
    return tuning_results.sort_values('Best CV Score', ascending=False)

X = data.drop(['concrete_compressive_strength'],axis=1)
y = data['concrete_compressive_strength']
tuning_results = hyperparameter_tuning(X, y)
tuning_results